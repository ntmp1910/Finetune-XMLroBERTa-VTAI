# Fine-tune XLM-RoBERTa cho Vietnamese Text Classification

Project nÃ y fine-tune model XLM-RoBERTa Ä‘á»ƒ phÃ¢n loáº¡i vÄƒn báº£n tiáº¿ng Viá»‡t vá»›i accuracy Ä‘áº¡t Ä‘Æ°á»£c 62.72%.

## ğŸš€ TÃ­nh nÄƒng

- Fine-tune XLM-RoBERTa cho Vietnamese text classification
- Há»— trá»£ format dá»¯ liá»‡u JSON/JSONL
- Tá»± Ä‘á»™ng xá»­ lÃ½ GPU/CPU
- Predict vá»›i confidence scores
- TÃ­ch há»£p wandb logging (offline mode)

## ğŸ“ Cáº¥u trÃºc Project

```
FineTune xlmr-roBerta/
â”œâ”€â”€ finetune_xlm_roberta.py      # Script chÃ­nh Ä‘á»ƒ fine-tune
â”œâ”€â”€ predict.py                   # Script Ä‘á»ƒ predict vá»›i model Ä‘Ã£ train
â”œâ”€â”€ test_texts.txt              # File test texts Ä‘á»ƒ predict
â”œâ”€â”€ run_finetune_fixed.sh       # Script cháº¡y fine-tune vá»›i env vars
â”œâ”€â”€ run_finetune_docker.sh      # Script cháº¡y fine-tune vá»›i Docker
â”œâ”€â”€ run_finetune_sudo.sh        # Script cháº¡y fine-tune vá»›i sudo
â”œâ”€â”€ data/
â”‚   â””â”€â”€ dantri.jsonl            # Dá»¯ liá»‡u training (JSONL format)
â”œâ”€â”€ model_output/               # ThÆ° má»¥c lÆ°u model Ä‘Ã£ train
â”œâ”€â”€ requirements.txt            # Dependencies
â””â”€â”€ README.md                   # File nÃ y
```

## ğŸ“Š Cáº¥u trÃºc Dá»¯ liá»‡u

### JSON/JSONL Format
```json
{
  "title": "TiÃªu Ä‘á» bÃ i viáº¿t",
  "summary": "TÃ³m táº¯t ná»™i dung",
  "category": "TÃªn category"
}
```

### VÃ­ dá»¥ JSONL:
```jsonl
{"title": "BÃ£o sá»‘ 3 Ä‘á»• bá»™", "summary": "Thiá»‡t háº¡i náº·ng ná»", "category": "Thá»i tiáº¿t"}
{"title": "GiÃ¡ vÃ ng tÄƒng", "summary": "Thá»‹ trÆ°á»ng sÃ´i Ä‘á»™ng", "category": "Kinh táº¿"}
```

## ğŸ› ï¸ CÃ i Ä‘áº·t

### 1. CÃ i Ä‘áº·t dependencies
```bash
pip install -r requirements.txt
```

### 2. Chuáº©n bá»‹ dá»¯ liá»‡u
Äáº·t file dá»¯ liá»‡u JSON/JSONL vÃ o thÆ° má»¥c `data/`

## ğŸš€ Sá»­ dá»¥ng

### 1. Fine-tune Model

#### Cháº¡y trá»±c tiáº¿p:
```bash
python finetune_xlm_roberta.py \
  --data data/dantri.jsonl \
  --output_dir model_output \
  --epochs 3 \
  --train_bs 8 \
  --eval_bs 8 \
  --lr 2e-5
```

#### Cháº¡y vá»›i script (khuyáº¿n nghá»‹):
```bash
chmod +x run_finetune_fixed.sh
./run_finetune_fixed.sh
```

#### Cháº¡y vá»›i Docker:
```bash
chmod +x run_finetune_docker.sh
./run_finetune_docker.sh
```

### 2. Predict vá»›i Model Ä‘Ã£ Train

#### Predict vá»›i texts máº·c Ä‘á»‹nh:
```bash
python predict.py --model_path ./model_output
```

#### Predict vá»›i texts cá»¥ thá»ƒ:
```bash
python predict.py --model_path ./model_output \
  --texts "BÃ£o sá»‘ 3 Ä‘á»• bá»™ vÃ o miá»n Trung" "GiÃ¡ vÃ ng hÃ´m nay tÄƒng máº¡nh"
```

#### Predict vá»›i file texts:
```bash
python predict.py --model_path ./model_output --file test_texts.txt
```

#### Predict vá»›i confidence scores:
```bash
python predict.py --model_path ./model_output --file test_texts.txt --with_confidence
```

## âš™ï¸ Tham sá»‘

### Fine-tune Parameters
- `--data`: ÄÆ°á»ng dáº«n file dá»¯ liá»‡u JSON/JSONL
- `--output_dir`: ThÆ° má»¥c lÆ°u model (default: `./model_output`)
- `--epochs`: Sá»‘ epoch train (default: 3)
- `--train_bs`: Batch size train (default: 8)
- `--eval_bs`: Batch size eval (default: 8)
- `--lr`: Learning rate (default: 2e-5)
- `--max_length`: Max sequence length (default: 256)

### Predict Parameters
- `--model_path`: ÄÆ°á»ng dáº«n Ä‘áº¿n model Ä‘Ã£ train
- `--texts`: CÃ¡c text cáº§n predict
- `--file`: File chá»©a texts (má»—i dÃ²ng má»™t text)
- `--with_confidence`: Hiá»ƒn thá»‹ confidence scores

## ğŸ”§ Cáº¥u hÃ¬nh GPU

### Tá»± Ä‘á»™ng detect GPU:
```bash
# Model sáº½ tá»± Ä‘á»™ng sá»­ dá»¥ng GPU náº¿u cÃ³
python finetune_xlm_roberta.py --data data/dantri.jsonl
```

### Chá»‰ Ä‘á»‹nh GPU cá»¥ thá»ƒ:
```bash
# Sá»­ dá»¥ng GPU 0
python finetune_xlm_roberta.py --data data/dantri.jsonl --gpu_device cuda:0

# Sá»­ dá»¥ng CPU
python finetune_xlm_roberta.py --data data/dantri.jsonl --gpu_device cpu
```

## ğŸ“ˆ Káº¿t quáº£

### VÃ­ dá»¥ Predictions
```
Text: "BÃ£o sá»‘ 3 Ä‘á»• bá»™ vÃ o miá»n Trung"
Prediction: "Thá»i tiáº¿t"

Text: "GiÃ¡ vÃ ng hÃ´m nay tÄƒng máº¡nh"
Prediction: "Kinh táº¿"

Text: "CÃ´ng nghá»‡ AI phÃ¡t triá»ƒn nhanh chÃ³ng"
Prediction: "CÃ´ng nghá»‡"
```

## ğŸ› Troubleshooting

### Lá»—i GPU khÃ´ng tÃ¬m tháº¥y:
```bash
# Kiá»ƒm tra GPU
nvidia-smi

# Cháº¡y vá»›i CPU
python finetune_xlm_roberta.py --gpu_device cpu
```

### Lá»—i NCCL:
```bash
# Sá»­ dá»¥ng script vá»›i environment variables
./run_finetune_fixed.sh
```

### Lá»—i Memory:
```bash
# Giáº£m batch size
python finetune_xlm_roberta.py --train_bs 4 --eval_bs 4
```

## ğŸ“ Logs

### Wandb Logging
- Model sá»­ dá»¥ng wandb offline mode
- Logs Ä‘Æ°á»£c lÆ°u táº¡i `wandb/offline-run-*`
- Sync logs: `wandb sync wandb/offline-run-*`

### Training Logs
- Model checkpoints: `model_output/checkpoint-*`
- Best model: `model_output/`
- Training logs: Console output

## ğŸ¤ ÄÃ³ng gÃ³p

1. Fork project
2. Táº¡o feature branch
3. Commit changes
4. Push to branch
5. Táº¡o Pull Request

## ğŸ“„ License

MIT License

## ğŸ“ LiÃªn há»‡

- Email: [your-email@example.com]
- GitHub: [your-github-username]

---

**LÆ°u Ã½**: Model Ä‘Æ°á»£c train trÃªn dá»¯ liá»‡u tiáº¿ng Viá»‡t vÃ  cÃ³ thá»ƒ cáº§n fine-tune thÃªm cho domain cá»¥ thá»ƒ. 