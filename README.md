# Fine-tune XLM-RoBERTa cho PhÃ¢n loáº¡i VÄƒn báº£n Tiáº¿ng Viá»‡t

Dá»± Ã¡n nÃ y fine-tune model XLM-RoBERTa Ä‘á»ƒ phÃ¢n loáº¡i vÄƒn báº£n tiáº¿ng Viá»‡t sá»­ dá»¥ng dá»¯ liá»‡u tá»« file JSON.

## ğŸ“ Cáº¥u trÃºc Dá»± Ã¡n

```
FineTune xlmr-roBerta/
â”œâ”€â”€ data_demo.json          # Dá»¯ liá»‡u JSON máº«u 1
â”œâ”€â”€ data_demo2.json         # Dá»¯ liá»‡u JSON máº«u 2  
â”œâ”€â”€ finetune_xlm_roberta.py # Script chÃ­nh Ä‘á»ƒ fine-tune
â”œâ”€â”€ quick_test.py           # Script test nhanh
â”œâ”€â”€ requirements.txt         # Dependencies
â”œâ”€â”€ analyze_data.py         # Script phÃ¢n tÃ­ch dá»¯ liá»‡u
â””â”€â”€ README.md              # HÆ°á»›ng dáº«n nÃ y
```

## ğŸš€ CÃ i Ä‘áº·t

1. **CÃ i Ä‘áº·t dependencies:**
```bash
pip install -r requirements.txt
```

2. **Kiá»ƒm tra mÃ´i trÆ°á»ng:**
```bash
python quick_test.py
```

## ğŸ“Š Cáº¥u trÃºc Dá»¯ liá»‡u

Dá»¯ liá»‡u JSON cáº§n cÃ³ cáº¥u trÃºc nhÆ° sau:

```json
[
  {
    "title": "TiÃªu Ä‘á» bÃ i bÃ¡o",
    "category": "Thá»ƒ thao",
    "summary": "TÃ³m táº¯t ná»™i dung bÃ i bÃ¡o",
    "datetime": "24/07/2025 12:30 GMT+7"
  }
]
```

### ï¸âœ… Há»— trá»£ JSONL (jsonlines/NDJSON)

Báº¡n cÅ©ng cÃ³ thá»ƒ dÃ¹ng file `.jsonl` (má»—i dÃ²ng lÃ  má»™t object JSON). Code Ä‘Ã£ tá»± Ä‘á»™ng nháº­n diá»‡n vÃ  load theo tá»«ng dÃ²ng:

VÃ­ dá»¥ `data.jsonl`:

```
{"title": "TiÃªu Ä‘á» 1", "category": "Thá»ƒ thao", "summary": "Ná»™i dung bÃ i 1"}
{"title": "TiÃªu Ä‘á» 2", "category": "PhÃ¡p luáº­t", "summary": "Ná»™i dung bÃ i 2"}
```

- TrÆ°á»ng báº¯t buá»™c: `title`, `category`, `summary`
- TrÆ°á»ng khÃ¡c (nhÆ° `datetime`) lÃ  tÃ¹y chá»n
- Cháº¡y fine-tune vá»›i JSONL chá»‰ cáº§n trá» `--data` tá»›i file `.jsonl`:

```bash
docker run --gpus all --rm -it \
  -e CUDA_VISIBLE_DEVICES="1" \
  -v $PWD:/work -w /work \
  pytorch/pytorch:2.1.0-cuda12.1-cudnn8-runtime \
  bash -lc "pip install -r requirements.txt && \
            python finetune_xlm_roberta.py \
              --data path/to/your_data.jsonl \
              --base_model xlm-roberta-base \
              --output_dir model_output"
```

### CÃ¡c trÆ°á»ng báº¯t buá»™c:
- `title`: TiÃªu Ä‘á» bÃ i bÃ¡o
- `category`: Danh má»¥c phÃ¢n loáº¡i
- `summary`: TÃ³m táº¯t ná»™i dung

## ğŸ”§ Sá»­ dá»¥ng

### 1. Test nhanh
```bash
python quick_test.py
```

### 2. Fine-tune model trÃªn GPU server (qua Docker)

- Cháº¡y fine-tune (chá»n GPU báº±ng CUDA_VISIBLE_DEVICES):

```bash
# VÃ­ dá»¥: dÃ¹ng GPU sá»‘ 1, data á»Ÿ data_demo2.json, lÆ°u ra ./model_output
docker run --gpus all --rm -it \
  -e CUDA_VISIBLE_DEVICES="1" \
  -v $PWD:/work -w /work \
  pytorch/pytorch:2.1.0-cuda12.1-cudnn8-runtime \
  bash -lc "pip install -r requirements.txt && \
            python finetune_xlm_roberta.py \
              --data data_demo2.json \
              --base_model xlm-roberta-base \
              --output_dir model_output \
              --epochs 3 --train_bs 8 --eval_bs 8 --lr 2e-5"
```

- Resume tá»« model/ckpt sáºµn cÃ³ (Ä‘Ã£ cÃ³ á»Ÿ thÆ° má»¥c lÃ m viá»‡c):

```bash
docker run --gpus all --rm -it \
  -e CUDA_VISIBLE_DEVICES="1" \
  -v $PWD:/work -w /work \
  pytorch/pytorch:2.1.0-cuda12.1-cudnn8-runtime \
  bash -lc "pip install -r requirements.txt && \
            python finetune_xlm_roberta.py \
              --data data_demo2.json \
              --base_model xlm-roberta-base \
              --resume_from model_output \
              --output_dir model_output \
              --epochs 1 --train_bs 8 --eval_bs 8 --lr 1e-5"
```

### 3. Sá»­ dá»¥ng model Ä‘Ã£ train

```python
from finetune_xlm_roberta import VietnameseTextClassifier

# Load model Ä‘Ã£ train
classifier = VietnameseTextClassifier()
classifier.setup_model()

# Load model Ä‘Ã£ lÆ°u
classifier.model = AutoModelForSequenceClassification.from_pretrained("./model_output")
classifier.tokenizer = AutoTokenizer.from_pretrained("./model_output")

# Dá»± Ä‘oÃ¡n
texts = [
    "Truy nÃ£ nghi pháº¡m lá»«a Ä‘áº£o 5,7 tá»‰ Ä‘á»“ng tiá»n Ä‘áº·t cá»c mua lÃºa",
    "Xung Ä‘á»™t ThÃ¡i Lan - Campuchia: ThÃ¡i Lan ghi nháº­n Ã­t nháº¥t 12 ngÆ°á»i thiá»‡t máº¡ng"
]

predictions = classifier.predict(texts)
for text, pred in zip(texts, predictions):
    print(f"Text: {text[:50]}...")
    print(f"Category: {pred}")
```

## âš™ï¸ Cáº¥u hÃ¬nh

### Hyperparameters cÃ³ thá»ƒ Ä‘iá»u chá»‰nh:

```python
# Trong finetune_xlm_roberta.py
classifier = VietnameseTextClassifier(
    model_name="xlm-roberta-base",  # CÃ³ thá»ƒ thay báº±ng "xlm-roberta-large"
    max_length=256                  # Äá»™ dÃ i tá»‘i Ä‘a cá»§a input
)

# Training arguments
training_args = TrainingArguments(
    learning_rate=2e-5,           # Learning rate
    per_device_train_batch_size=8, # Batch size
    num_train_epochs=3,           # Sá»‘ epochs
    weight_decay=0.01,            # Weight decay
    warmup_steps=500,             # Warmup steps
)
```

## ğŸ“ˆ Káº¿t quáº£

Script sáº½:
1. **PhÃ¢n tÃ­ch dá»¯ liá»‡u**: Hiá»ƒn thá»‹ thá»‘ng kÃª vá» categories vÃ  phÃ¢n bá»‘
2. **Chia dá»¯ liá»‡u**: Train (70%), Validation (10%), Test (20%)
3. **Fine-tune model**: Training vá»›i XLM-RoBERTa
4. **ÄÃ¡nh giÃ¡**: Accuracy vÃ  classification report
5. **LÆ°u model**: Táº¡i thÆ° má»¥c `./model_output`

## ğŸ¯ CÃ¡c Categories Ä‘Æ°á»£c phÃ¡t hiá»‡n

Tá»« dá»¯ liá»‡u `data_demo2.json`, cÃ¡c categories bao gá»“m:
- Thá»i sá»±
- Thá»ƒ thao  
- PhÃ¡p luáº­t
- Tháº¿ giá»›i
- Xe
- Sá»©c khá»e
- GiÃ¡o dá»¥c
- Giáº£i trÃ­
- VÃ  nhiá»u categories khÃ¡c...

## ğŸ” Troubleshooting

### Lá»—i thÆ°á»ng gáº·p:

1. **Out of Memory (OOM)**:
   - Giáº£m `max_length` xuá»‘ng 128 hoáº·c 256
   - Giáº£m `per_device_train_batch_size` xuá»‘ng 4 hoáº·c 2
   - Sá»­ dá»¥ng gradient accumulation

2. **CUDA not available**:
   - CÃ i Ä‘áº·t PyTorch vá»›i CUDA: `pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118`

3. **Dá»¯ liá»‡u khÃ´ng Ä‘Ãºng format**:
   - Kiá»ƒm tra cáº¥u trÃºc JSON vá»›i `python quick_test.py`

## ğŸ“ Ghi chÃº

- Model XLM-RoBERTa há»— trá»£ Ä‘a ngÃ´n ngá»¯, phÃ¹ há»£p cho tiáº¿ng Viá»‡t
- Dá»¯ liá»‡u training cáº§n cÃ¢n báº±ng giá»¯a cÃ¡c categories Ä‘á»ƒ trÃ¡nh bias
- CÃ³ thá»ƒ sá»­ dá»¥ng data augmentation Ä‘á»ƒ tÄƒng sá»‘ lÆ°á»£ng máº«u training
- Model Ä‘Æ°á»£c lÆ°u táº¡i `./model_output/` sau khi training xong

## ğŸ¤ ÄÃ³ng gÃ³p

Náº¿u báº¡n muá»‘n cáº£i thiá»‡n dá»± Ã¡n nÃ y, hÃ£y:
1. Fork repository
2. Táº¡o feature branch
3. Commit changes
4. Push to branch
5. Táº¡o Pull Request

## ğŸ“„ License

Dá»± Ã¡n nÃ y Ä‘Æ°á»£c phÃ¡t hÃ nh dÆ°á»›i MIT License. 