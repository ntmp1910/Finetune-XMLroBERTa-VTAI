# Fine-tune XLM-RoBERTa cho Vietnamese Text Classification

Project nÃ y fine-tune model XLM-RoBERTa Ä‘á»ƒ phÃ¢n loáº¡i vÄƒn báº£n tiáº¿ng Viá»‡t vá»›i accuracy Ä‘áº¡t Ä‘Æ°á»£c 62.72%.

## ğŸš€ TÃ­nh nÄƒng

- Fine-tune XLM-RoBERTa cho Vietnamese text classification
- Há»— trá»£ format dá»¯ liá»‡u JSON/JSONL
- Tá»± Ä‘á»™ng xá»­ lÃ½ GPU/CPU
- Predict vá»›i confidence scores
- TÃ­ch há»£p wandb logging (offline mode)

## ğŸ“ Cáº¥u trÃºc Project

```
FineTune xlmr-roBerta/
â”œâ”€â”€ finetune_xlm_roberta.py      # Script chÃ­nh Ä‘á»ƒ fine-tune (JSON/JSONL)
â”œâ”€â”€ finetune_xlm_roberta_txt.py  # Script fine-tune vá»›i dá»¯ liá»‡u TXT
â”œâ”€â”€ predict.py                   # Script predict vá»›i model JSON/JSONL
â”œâ”€â”€ predict_txt.py              # Script predict vá»›i model TXT
â”œâ”€â”€ test_texts.txt              # File test texts Ä‘á»ƒ predict
â”œâ”€â”€ input_texts.txt             # File input texts Ä‘á»ƒ predict
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ dantri.jsonl            # Dá»¯ liá»‡u training (JSONL format)
â”‚   â””â”€â”€ sample_data.txt         # Dá»¯ liá»‡u training (TXT format)
â”œâ”€â”€ model_output/               # ThÆ° má»¥c lÆ°u model JSON/JSONL
â”œâ”€â”€ model_output_txt/           # ThÆ° má»¥c lÆ°u model TXT
â”œâ”€â”€ requirements.txt            # Dependencies
â””â”€â”€ README.md                   # File nÃ y
```

## ğŸ“Š Cáº¥u trÃºc Dá»¯ liá»‡u

### JSON/JSONL Format
```json
{
  "title": "TiÃªu Ä‘á» bÃ i viáº¿t",
  "summary": "TÃ³m táº¯t ná»™i dung",
  "category": "TÃªn category"
}
```

### VÃ­ dá»¥ JSONL:
```jsonl
{"title": "BÃ£o sá»‘ 3 Ä‘á»• bá»™", "summary": "Thiá»‡t háº¡i náº·ng ná»", "category": "Thá»i tiáº¿t"}
{"title": "GiÃ¡ vÃ ng tÄƒng", "summary": "Thá»‹ trÆ°á»ng sÃ´i Ä‘á»™ng", "category": "Kinh táº¿"}
```

### TXT Format
Format: `text\tlabel` hoáº·c `text|label` hoáº·c `text,label`

### VÃ­ dá»¥ TXT:
```
BÃ£o sá»‘ 3 Ä‘á»• bá»™ vÃ o miá»n Trung gÃ¢y thiá»‡t háº¡i náº·ng ná»	Thá»i tiáº¿t
GiÃ¡ vÃ ng hÃ´m nay tÄƒng máº¡nh lÃªn má»©c cao nháº¥t trong thÃ¡ng	Kinh táº¿
CÃ´ng nghá»‡ AI phÃ¡t triá»ƒn nhanh chÃ³ng trong nÄƒm 2024	CÃ´ng nghá»‡
```

## ğŸ› ï¸ CÃ i Ä‘áº·t

### 1. CÃ i Ä‘áº·t dependencies
```bash
pip install -r requirements.txt
```

### 2. Chuáº©n bá»‹ dá»¯ liá»‡u
Äáº·t file dá»¯ liá»‡u JSON/JSONL hoáº·c TXT vÃ o thÆ° má»¥c `data/`

### 3. Chuáº©n bá»‹ file input cho predict (tÃ¹y chá»n)
Táº¡o file text chá»©a cÃ¡c cÃ¢u cáº§n predict (má»—i dÃ²ng má»™t cÃ¢u) nhÆ° `input_texts.txt`

## ğŸš€ Sá»­ dá»¥ng

### 1. Fine-tune Model vá»›i JSON/JSONL

#### Cháº¡y trá»±c tiáº¿p:
```bash
python finetune_xlm_roberta.py \
  --data data/dantri.jsonl \
  --output_dir model_output \
  --epochs 3 \
  --train_bs 8 \
  --eval_bs 8 \
  --lr 2e-5
```

#### Cháº¡y vá»›i environment variables (khuyáº¿n nghá»‹):
```bash
# Set environment variables Ä‘á»ƒ trÃ¡nh lá»—i
export TOKENIZERS_PARALLELISM=false
export NCCL_DEBUG=INFO
export CUDA_LAUNCH_BLOCKING=1

# Cháº¡y fine-tune
python finetune_xlm_roberta.py \
  --data data/dantri.jsonl \
  --output_dir model_output \
  --epochs 3 \
  --train_bs 4 \
  --eval_bs 4 \
  --lr 2e-5
```

### 2. Fine-tune Model vá»›i TXT

#### Cháº¡y trá»±c tiáº¿p:
```bash
python finetune_xlm_roberta_txt.py \
  --data data/sample_data.txt \
  --delimiter "\t" \
  --output_dir model_output_txt \
  --epochs 3 \
  --train_bs 8 \
  --eval_bs 8 \
  --lr 2e-5
```

#### Cháº¡y vá»›i environment variables:
```bash
# Set environment variables Ä‘á»ƒ trÃ¡nh lá»—i
export TOKENIZERS_PARALLELISM=false
export NCCL_DEBUG=INFO
export CUDA_LAUNCH_BLOCKING=1

# Cháº¡y fine-tune vá»›i TXT
python finetune_xlm_roberta_txt.py \
  --data data/sample_data.txt \
  --delimiter "\t" \
  --output_dir model_output_txt \
  --epochs 3 \
  --train_bs 4 \
  --eval_bs 4 \
  --lr 2e-5
```

### 3. Predict vá»›i Model Ä‘Ã£ Train

#### Predict vá»›i model JSON/JSONL:
```bash
python predict.py --model_path ./model_output
```

#### Predict vá»›i model TXT:
```bash
python predict_txt.py --model_path ./model_output_txt
```

#### Predict vá»›i texts cá»¥ thá»ƒ:
```bash
# JSON/JSONL model
python predict.py --model_path ./model_output \
  --texts "BÃ£o sá»‘ 3 Ä‘á»• bá»™ vÃ o miá»n Trung" "GiÃ¡ vÃ ng hÃ´m nay tÄƒng máº¡nh"

# TXT model
python predict_txt.py --model_path ./model_output_txt \
  --texts "BÃ£o sá»‘ 3 Ä‘á»• bá»™ vÃ o miá»n Trung" "GiÃ¡ vÃ ng hÃ´m nay tÄƒng máº¡nh"
```

#### Predict vá»›i file texts:
```bash
# JSON/JSONL model
python predict.py --model_path ./model_output --file input_texts.txt

# TXT model
python predict_txt.py --model_path ./model_output_txt --file input_texts.txt
```

#### Predict vá»›i confidence scores:
```bash
# JSON/JSONL model
python predict.py --model_path ./model_output --file input_texts.txt --with_confidence

# TXT model
python predict_txt.py --model_path ./model_output_txt --file input_texts.txt --with_confidence
```

## âš™ï¸ Tham sá»‘

### Fine-tune Parameters (JSON/JSONL)
- `--data`: ÄÆ°á»ng dáº«n file dá»¯ liá»‡u JSON/JSONL
- `--output_dir`: ThÆ° má»¥c lÆ°u model (default: `./model_output`)
- `--epochs`: Sá»‘ epoch train (default: 3)
- `--train_bs`: Batch size train (default: 8)
- `--eval_bs`: Batch size eval (default: 8)
- `--lr`: Learning rate (default: 2e-5)
- `--max_length`: Max sequence length (default: 256)

### Fine-tune Parameters (TXT)
- `--data`: ÄÆ°á»ng dáº«n file dá»¯ liá»‡u TXT
- `--delimiter`: Delimiter giá»¯a text vÃ  label (default: `\t`)
- `--output_dir`: ThÆ° má»¥c lÆ°u model (default: `./model_output_txt`)
- `--epochs`: Sá»‘ epoch train (default: 3)
- `--train_bs`: Batch size train (default: 8)
- `--eval_bs`: Batch size eval (default: 8)
- `--lr`: Learning rate (default: 2e-5)
- `--max_length`: Max sequence length (default: 256)

### Predict Parameters
- `--model_path`: ÄÆ°á»ng dáº«n Ä‘áº¿n model Ä‘Ã£ train
- `--texts`: CÃ¡c text cáº§n predict
- `--file`: File chá»©a texts (má»—i dÃ²ng má»™t text)
- `--with_confidence`: Hiá»ƒn thá»‹ confidence scores

## ğŸ”§ Cáº¥u hÃ¬nh GPU

### Tá»± Ä‘á»™ng detect GPU:
```bash
# Model sáº½ tá»± Ä‘á»™ng sá»­ dá»¥ng GPU náº¿u cÃ³
python finetune_xlm_roberta.py --data data/dantri.jsonl
```

### Chá»‰ Ä‘á»‹nh GPU cá»¥ thá»ƒ:
```bash
# Sá»­ dá»¥ng GPU 0
python finetune_xlm_roberta.py --data data/dantri.jsonl --gpu_device cuda:0

# Sá»­ dá»¥ng CPU
python finetune_xlm_roberta.py --data data/dantri.jsonl --gpu_device cpu
```

## ğŸ“ˆ Káº¿t quáº£

### VÃ­ dá»¥ Predictions
```
Text: "BÃ£o sá»‘ 3 Ä‘á»• bá»™ vÃ o miá»n Trung"
Prediction: "Thá»i tiáº¿t"

Text: "GiÃ¡ vÃ ng hÃ´m nay tÄƒng máº¡nh"
Prediction: "Kinh táº¿"

Text: "CÃ´ng nghá»‡ AI phÃ¡t triá»ƒn nhanh chÃ³ng"
Prediction: "CÃ´ng nghá»‡"
```

## ğŸ› Troubleshooting

### Lá»—i GPU khÃ´ng tÃ¬m tháº¥y:
```bash
# Kiá»ƒm tra GPU
nvidia-smi

# Cháº¡y vá»›i CPU
python finetune_xlm_roberta.py --gpu_device cpu
```

### Lá»—i NCCL:
```bash
# Sá»­ dá»¥ng environment variables
export TOKENIZERS_PARALLELISM=false
export NCCL_DEBUG=INFO
export CUDA_LAUNCH_BLOCKING=1
python finetune_xlm_roberta.py --data data/dantri.jsonl
```

### Lá»—i Memory:
```bash
# Giáº£m batch size
python finetune_xlm_roberta.py --train_bs 4 --eval_bs 4
```

## ğŸ“ Logs

### Wandb Logging
- Model sá»­ dá»¥ng wandb offline mode
- Logs Ä‘Æ°á»£c lÆ°u táº¡i `wandb/offline-run-*`
- Sync logs: `wandb sync wandb/offline-run-*`

### Training Logs
- Model checkpoints: `model_output/checkpoint-*`
- Best model: `model_output/`
- Training logs: Console output

## ğŸ¤ ÄÃ³ng gÃ³p

1. Fork project
2. Táº¡o feature branch
3. Commit changes
4. Push to branch
5. Táº¡o Pull Request

## ğŸ“„ License

MIT License

## ğŸ“ LiÃªn há»‡

- Email: [ntmpwork@example.com]
- GitHub: [ntmp1910]

---

**LÆ°u Ã½**: Model Ä‘Æ°á»£c train trÃªn dá»¯ liá»‡u tiáº¿ng Viá»‡t vÃ  cÃ³ thá»ƒ cáº§n fine-tune thÃªm cho domain cá»¥ thá»ƒ. 